---
title: "Milestone Report - Data Science Specialisation"
author: "Michael Bristow"
date: "28 December 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
This report outlines the objective of this projects and looks at the steps taken towards achieving this goal.
It will specifically look at

- Data sources used for building prediction model
- Basic data summary
- Steps taken to produce sample files and cleaning of data
- Basic data exploration
- Producing n-gram's from data
- Outlinje of further work to be done

##Data Sources
Initial data was downloaded from
https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

This produced three files that were tobe used for producing the n-gram model

- en_US.blogs
- en_US.news
- en_US.twitter

##Data Summary
```r
| File    | Size(Mb) |     Lines | Longest line | Words      |
|---------|:--------:|----------:|--------------|------------|
| blogs   |  200.424 |   899,288 | 40,833       | 37,334,131 |
| news    |  196.278 | 1,010,242 | 11,384       | 34,372,530 |
| twitter |  159.364 | 2,360,148 | 140          | 3,0373,583 |
```
##Sampling
As can be seen these are very largew files containing over 4m text lines.

Sample files were created to speed up the analysis.

Initially they were samples at a rate of only 10%. This may be increased depending on performance of the model developed.

This was done using a binomial distribution 

##Cleaning of sampled data
The data was then "cleaned" by

- converting to lower case
- removing punctuation and numbers
- removing "stop" words
- removing any white space
- stem words

Sample file was saved as RDS to allow for more efficient opening in future  tasks

##Data exploration
The data was sorted by the highest occurence of each word

The following wordcloud was produced

```{r echo = FALSE, message=FALSE, results='asis', warning = FALSE}
library(tm)
library(wordcloud)
library(dplyr)
library(tidytext)

masterPath <- "./datadir/final/en_US/"
cleanCorpus <- paste(masterPath,"cleanCorpus", ".RDS", sep = "")
finalCorpus <- readRDS(cleanCorpus)
wordcloud(finalCorpus, max.words = 100, random.order = FALSE,  colors=brewer.pal(8, "Dark2"))
```

Here is a histogram showing the 10 highest word occurrences
```{r echo = FALSE, message=FALSE, results='asis', warning = FALSE}
#creaet plt function
plotNgram <- function(myNgram, topNum){
  ap_td <- tidy(finalCorpus)
  
  myout <- ap_td %>%
    group_by(term) %>%
      summarise(tot_count =sum(count)) %>%
        arrange(desc(tot_count))
  
  myData <- myout[1:topNum,]
  myData$term2 <- factor(myData$term, as.character(myData$term))
  
  
  histo <- ggplot(myData, aes(x=term2, y= tot_count)) + 
    geom_bar(stat="identity") +
    xlab("Words") +
    ylab("Occurrence") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  histo
}
```



```{r echo = FALSE, message=FALSE, results='asis', warning = FALSE}
library(ggplot2)
cleanCorpus <- paste(masterPath,"cleanCorpus1ngram", ".RDS", sep = "")
finalCorpus <- readRDS(cleanCorpus)

g1 <- plotNgram(finalCorpus,10)
print(g1)
```

##N-Grams
We use ngrams in our prediction model. An ngram is a list of continmuous words "n" long

a 2-gram is a list of two words

These are created using the RWeka library.

This a list of the top 10 2-gram


```{r echo = FALSE, message=FALSE, results='asis', warning = FALSE}

cleanCorpus <- paste(masterPath,"cleanCorpus2ngram", ".RDS", sep = "")
finalCorpus <- readRDS(cleanCorpus)

g2 <- plotNgram(finalCorpus,10)

print(g2)
```

This a list of the top 10 3-gram


```{r echo = FALSE, message=FALSE, results='asis', warning = FALSE}
cleanCorpus <- paste(masterPath,"cleanCorpus3ngram", ".RDS", sep = "")
finalCorpus <- readRDS(cleanCorpus)

g3 <- plotNgram(finalCorpus,10)

print(g3)
```

##Findings and Conclusion

- The data consists of three files of approximately 200mb each
- A sample of 10% was taken, but this csan be adjusted for performancer and accuracy

###Next step
- The next steps ini the project are to design and implement an algorithym to predict then next word
- This wil be done via a "shiny" application

